1. Forrest et al. describe the Random Forest method for population mapping. They draw upon correlations between land cover features and population, plus national census data, and use these data to fit all of the people to their respective place. Their algorithm reads pixel input to generate regression "trees" in order to predict the population. However, this algorithm likely needs to be trained for each country, as different factors matter more in different countries. For example, Kenya's population is most predicted by distance from healthcare centers.

2. A machine-learning algorithm is an algorithm based around a loss function and which interprets inputs in accord to that loss function. Based on how its performances are rated by a controller, the algorithm will adjust its weights (the slope of a line running through vector space) and its biases (the displacement of the line running through vector space) until it correctly sorts the input and output. Machine learning has a heavy statistical element to it, and I would argue is just an automation of classical analysis methods. The primary difference is that a human can see unsorted data where a computer will try to force a pattern.

3. The Random Forest model incorporates night-time light, topography, land cover, and clinical information. I am guessing that each of these sets would be approximately the same number of grid cells, and thus the general formula should be covariates times cells. Data analysts are integral in choosing these covariates, as unnecessary covariates can either drastically increase computational complexity, or give nonsensical patterns that don't actually exist and ruin the data. When given meaningless data, the computer will still try to force a pattern; it knows nothing else.

4. Cynically, the importance of having an accurate picture of population distribution is so that companies know where to find workers and consumers. Also it's a good way to keep people under control, if you always know where they are. Not-so-cynically, having an accurate picture can help distribute resources and plan for disasters. 

I would like to use the data from my LMIC to generate a machine learning algorithm that predicts population density based on all of the dimensions that we're using in project 2 (once the CS lab gives me access to tensorflow and docker). I would then apply that algorithm to another country with a similar climate. Having rigorously-obtained data would help in checking to see if I trained it properly. I guess I would also like to add that having this sort of thing be the norm may eventually result in massively inaccurate numbers. If we keep on generating statistically-accurate spreads of population, then we're probably generating them from other statistically-accurate spreads of population. Eventually, the error should accrue and the probability of us getting a horribly-biased set will rapidly approach 1. 
